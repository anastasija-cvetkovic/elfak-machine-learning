![Revisit consent button](https://cdn-cookieyes.com/assets/images/revisit.svg)

We value your privacy

We use cookies to enhance your browsing experience, serve personalised ads or content, and analyse our traffic. By clicking "Accept All", you consent to our use of cookies.

CustomiseReject AllAccept All

Powered by [Visit CookieYes website](https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=fl-branding)

Customise Consent Preferences![](https://cdn-cookieyes.com/assets/images/close.svg)

We use cookies to help you navigate efficiently and perform certain functions. You will find detailed information about all cookies under each consent category below.

The cookies that are categorised as "Necessary" are stored on your browser as they are essential for enabling the basic functionalities of the site. ... Show more

NecessaryAlways Active

Necessary cookies are required to enable the basic features of this site, such as providing secure log-in or adjusting your consent preferences. These cookies do not store any personally identifiable data.

- Cookie

\_\_cf\_bm

- Duration

1 hour

- Description

This cookie, set by Cloudflare, is used to support Cloudflare Bot Management.


- Cookie

AWSALBCORS

- Duration

7 days

- Description

Amazon Web Services set this cookie for load balancing.


- Cookie

\_cfuvid

- Duration

session

- Description

Cloudflare sets this cookie to track users across sessions to optimize user experience by maintaining session consistency and providing personalized services


- Cookie

li\_gc

- Duration

6 months

- Description

Linkedin set this cookie for storing visitor's consent regarding using cookies for non-essential purposes.


- Cookie

\_\_hssrc

- Duration

session

- Description

This cookie is set by Hubspot whenever it changes the session cookie. The \_\_hssrc cookie set to 1 indicates that the user has restarted the browser, and if the cookie does not exist, it is assumed to be a new session.


- Cookie

\_\_hssc

- Duration

1 hour

- Description

HubSpot sets this cookie to keep track of sessions and to determine if HubSpot should increment the session number and timestamps in the \_\_hstc cookie.


- Cookie

wpEmojiSettingsSupports

- Duration

session

- Description

WordPress sets this cookie when a user interacts with emojis on a WordPress site. It helps determine if the user's browser can display emojis properly.


- Cookie

\_octo

- Duration

1 year

- Description

No description available.


- Cookie

logged\_in

- Duration

1 year

- Description

No description available.


- Cookie

\_\_Secure-YEC

- Duration

past

- Description

YouTube sets this cookie to stores the user's video player preferences using embedded YouTube video


- Cookie

\_\_eoi

- Duration

6 months

- Description

Description is currently not available.


- Cookie

AWSALBTGCORS

- Duration

7 days

- Description

No description available.


- Cookie

login-status-p

- Duration

past

- Description

Description is currently not available.


- Cookie

AWSALBTG

- Duration

7 days

- Description

No description available.


- Cookie

csrf\_token

- Duration

session

- Description

No description available.


- Cookie

token\_v2

- Duration

1 day

- Description

Description is currently not available.


- Cookie

D

- Duration

1 year

- Description

Description is currently not available.


- Cookie

PHPSESSID

- Duration

session

- Description

This cookie is native to PHP applications. The cookie stores and identifies a user's unique session ID to manage user sessions on the website. The cookie is a session cookie and will be deleted when all the browser windows are closed.


- Cookie

VISITOR\_PRIVACY\_METADATA

- Duration

6 months

- Description

YouTube sets this cookie to store the user's cookie consent state for the current domain.


- Cookie

cookietest

- Duration

session

- Description

The cookietest cookie is typically used to determine whether the user's browser accepts cookies, essential for website functionality and user experience.


- Cookie

\_\_Host-airtable-session

- Duration

1 year

- Description

This cookie is used to enable us to integrate the services of Airtable.


- Cookie

\_\_Host-airtable-session.sig

- Duration

1 year

- Description

This cookie is used to enable us to integrate the services of Airtable.


- Cookie

m

- Duration

1 year 1 month 4 days

- Description

Stripe sets this cookie for fraud prevention purposes. It identifies the device used to access the website, allowing the website to be formatted accordingly.


- Cookie

BIGipServer\*

- Duration

session

- Description

Marketo sets this cookie to collect information about the user's online activity and build a profile about their interests to provide advertisements relevant to the user.


- Cookie

\_\_cfruid

- Duration

session

- Description

Cloudflare sets this cookie to identify trusted web traffic.


- Cookie

\_GRECAPTCHA

- Duration

6 months

- Description

Google Recaptcha service sets this cookie to identify bots to protect the website against malicious spam attacks.


- Cookie

\_\_Secure-YNID

- Duration

6 months

- Description

Google cookie used to protect user security and prevent fraud, especially during the login process.


- Cookie

cookieyes-consent

- Duration

1 year

- Description

CookieYes sets this cookie to remember users' consent preferences so that their preferences are respected on subsequent visits to this site. It does not collect or store any personal information about the site visitors.


Functional

Functional cookies help perform certain functionalities like sharing the content of the website on social media platforms, collecting feedback, and other third-party features.

- Cookie

BCTempID

- Duration

10 minutes

- Description

No description available.


- Cookie

BCSessionID

- Duration

1 year 1 month 4 days

- Description

Blueconic sets this cookie as a unique identifier for the BlueConic profile.


- Cookie

lidc

- Duration

1 day

- Description

LinkedIn sets the lidc cookie to facilitate data center selection.


- Cookie

brw

- Duration

1 year

- Description

No description available.


- Cookie

brwConsent

- Duration

5 minutes

- Description

Description is currently not available.


- Cookie

WMF-Uniq

- Duration

1 year

- Description

Description is currently not available.


- Cookie

loom\_anon\_comment

- Duration

1 year

- Description

No description available.


- Cookie

loom\_referral\_video

- Duration

session

- Description

Description is currently not available.


- Cookie

VISITOR\_INFO1\_LIVE

- Duration

6 months

- Description

A cookie set by YouTube to measure bandwidth that determines whether the user gets the new or old player interface.


- Cookie

yt-remote-connected-devices

- Duration

Never Expires

- Description

YouTube sets this cookie to store the user's video preferences using embedded YouTube videos.


- Cookie

ytidb::LAST\_RESULT\_ENTRY\_KEY

- Duration

Never Expires

- Description

The cookie ytidb::LAST\_RESULT\_ENTRY\_KEY is used by YouTube to store the last search result entry that was clicked by the user. This information is used to improve the user experience by providing more relevant search results in the future.


- Cookie

yt-remote-device-id

- Duration

Never Expires

- Description

YouTube sets this cookie to store the user's video preferences using embedded YouTube videos.


- Cookie

yt-remote-session-name

- Duration

session

- Description

The yt-remote-session-name cookie is used by YouTube to store the user's video player preferences using embedded YouTube video.


- Cookie

yt-remote-fast-check-period

- Duration

session

- Description

The yt-remote-fast-check-period cookie is used by YouTube to store the user's video player preferences for embedded YouTube videos.


- Cookie

yt-remote-session-app

- Duration

session

- Description

The yt-remote-session-app cookie is used by YouTube to store user preferences and information about the interface of the embedded YouTube video player.


- Cookie

yt-remote-cast-available

- Duration

session

- Description

The yt-remote-cast-available cookie is used to store the user's preferences regarding whether casting is available on their YouTube video player.


- Cookie

yt-remote-cast-installed

- Duration

session

- Description

The yt-remote-cast-installed cookie is used to store the user's video player preferences using embedded YouTube video.


- Cookie

cp\_session

- Duration

3 months

- Description

Codepen sets this cookie for Help systems found in the website.


- Cookie

loid

- Duration

1 year 1 month 4 days

- Description

This cookie is set by the Reddit. The cookie enables the sharing of content from the website onto the social media platform.


Analytics

Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics such as the number of visitors, bounce rate, traffic source, etc.

- Cookie

\_\_hstc

- Duration

6 months

- Description

Hubspot set this main cookie for tracking visitors. It contains the domain, initial timestamp (first visit), last timestamp (last visit), current timestamp (this visit), and session number (increments for each subsequent session).


- Cookie

hubspotutk

- Duration

6 months

- Description

HubSpot sets this cookie to keep track of the visitors to the website. This cookie is passed to HubSpot on form submission and used when deduplicating contacts.


- Cookie

\_ga

- Duration

1 year 1 month 4 days

- Description

Google Analytics sets this cookie to calculate visitor, session and campaign data and track site usage for the site's analytics report. The cookie stores information anonymously and assigns a randomly generated number to recognise unique visitors.


- Cookie

\_ga\_\*

- Duration

1 year 1 month 4 days

- Description

Google Analytics sets this cookie to store and count page views.


- Cookie

\_\_Host-psifi.analyticsTrace

- Duration

6 hours

- Description

Description is currently not available.


- Cookie

\_\_Host-psifi.analyticsTraceV2

- Duration

6 hours

- Description

Description is currently not available.


- Cookie

\_gh\_sess

- Duration

session

- Description

GitHub sets this cookie for temporary application and framework state between pages like what step the user is on in a multiple step form.


- Cookie

YSC

- Duration

session

- Description

YSC cookie is set by Youtube and is used to track the views of embedded videos on Youtube pages.


- Cookie

ajs\_anonymous\_id

- Duration

1 year

- Description

This cookie is set by Segment to count the number of people who visit a certain site by tracking if they have visited before.


- Cookie

vuid

- Duration

1 year 1 month 4 days

- Description

Vimeo installs this cookie to collect tracking information by setting a unique ID to embed videos on the website.


Performance

Performance cookies are used to understand and analyse the key performance indexes of the website which helps in delivering a better user experience for the visitors.

- Cookie

AWSALB

- Duration

7 days

- Description

AWSALB is an application load balancer cookie set by Amazon Web Services to map the session to the target.


- Cookie

acq

- Duration

past

- Description

Description is currently not available.


- Cookie

acq.sig

- Duration

past

- Description

Description is currently not available.


- Cookie

ptc

- Duration

2 years

- Description

No description available.


Advertisement

Advertisement cookies are used to provide visitors with customised advertisements based on the pages you visited previously and to analyse the effectiveness of the ad campaigns.

- Cookie

muc\_ads

- Duration

1 year 1 month 4 days

- Description

Twitter sets this cookie to collect user behaviour and interaction data to optimize the website.


- Cookie

guest\_id\_marketing

- Duration

1 year 1 month 4 days

- Description

Twitter sets this cookie to identify and track the website visitor.


- Cookie

guest\_id\_ads

- Duration

1 year 1 month 4 days

- Description

Twitter sets this cookie to identify and track the website visitor.


- Cookie

personalization\_id

- Duration

1 year 1 month 4 days

- Description

Twitter sets this cookie to integrate and share features for social media and also store information about how the user uses the website, for tracking and targeting.


- Cookie

guest\_id

- Duration

1 year 1 month 4 days

- Description

Twitter sets this cookie to identify and track the website visitor. It registers if a user is signed in to the Twitter platform and collects information about ad preferences.


- Cookie

bcookie

- Duration

1 year

- Description

LinkedIn sets this cookie from LinkedIn share buttons and ad tags to recognize browser IDs.


- Cookie

\_\_Secure-ROLLOUT\_TOKEN

- Duration

6 months

- Description

YouTube sets this cookie to manage feature rollout and experimentation. It helps Google control which new features or interface changes are shown to users as part of testing and staged rollouts, ensuring consistent experience for a given user during an experiment.


- Cookie

yt.innertube::nextId

- Duration

Never Expires

- Description

YouTube sets this cookie to register a unique ID to store data on what videos from YouTube the user has seen.


- Cookie

yt.innertube::requests

- Duration

Never Expires

- Description

YouTube sets this cookie to register a unique ID to store data on what videos from YouTube the user has seen.


- Cookie

session\_tracker

- Duration

session

- Description

This cookie is set by the Reddit. This cookie is used to identify trusted web traffic. It also helps in adverstising on the website.


- Cookie

edgebucket

- Duration

session

- Description

Reddit sets this cookie to save the information about a log-on Reddit user, for the purpose of advertisement recommendations and updating the content.


- Cookie

did

- Duration

1 year

- Description

Arbor sets this cookie to show targeted ads to site visitors.This cookie expires after 2 months or 1 year.


Uncategorised

Other uncategorised cookies are those that are being analysed and have not been classified into a category as yet.

No cookies to display.

Reject AllSave My PreferencesAccept All

Powered by [Visit CookieYes website](https://www.cookieyes.com/product/cookie-consent/?ref=cypbcyb&utm_source=cookie-banner&utm_medium=sl-branding)

[Skip to content](https://towardsdatascience.com/unsupervised-learning-project-creating-customer-segments-17c4b4bbf925/#wp--skip-link--target)

[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)

# Unsupervised Learning Project: Creating Customer Segments

Learn how to develop and end-to-end Clustering and Dimensionality Reduction Project!

[Victor Roman](https://towardsdatascience.com/author/rromanss23/)

Apr 28, 2019

16 min read

Share

![Picture from Unsplash](https://towardsdatascience.com/wp-content/uploads/2019/04/0-_0EwViz18k_3YGT.jpg)Picture from [Unsplash](https://unsplash.com/photos/5fNmWej4tAA)

## Introduction

Throughout this project, we are going to analyze the spending behaviours of several customers in some product categories. The main goals of the project are:

- Grouping customers in clusters of similar spending characteristics.
- Describing the variations within the different clusters, in order to find the best delivery structure for each group.

To carry out this project we will use the dataset that can be found in the following [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).

You can find the complete project, documentation and dataset on my [GitHub page](https://github.com/rromanss23/Machine_Leaning_Engineer_Udacity_NanoDegree/tree/master/projects/customer_segments):

[https://github.com/rromanss23/Machine\_Leaning\_Engineer\_Udacity\_NanoDegree/tree/master/projects/customer\_segments](https://github.com/rromanss23/Machine_Leaning_Engineer_Udacity_NanoDegree/tree/master/projects/customer_segments)

We will focus our analysis on the six product categories recorded for the customers, excluding the ‘Channel’ and ‘Region’ fields.

```
# Import libraries necessary for this project
import numpy as np
import pandas as pd
from IPython.display import display # Allows the use of display() for DataFrames

# Import supplementary visualizations code visuals.py
import visuals as vs

# Pretty display for notebooks
%matplotlib inline

# Load the wholesale customers dataset
try:
    data = pd.read_csv("customers.csv")
    data.drop(['Region', 'Channel'], axis = 1, inplace = True)
    print("Wholesale customers dataset has {} samples with {} features each.".format(*data.shape))
except:
    print("Dataset could not be loaded. Is the dataset missing?")
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/18qrOYg3rJMZEc_kx9WSEHA.png)

## Data Exploration

Now we will explore the dataset through visualizations and code in order to understand the relationships between features. In addition, we will calculate a statistical description of the dataset and consider the overall relevance of each feature.

```
# Display a description of the dataset
display(data.describe())
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1cvZ3pVLkGcUc0gZDh1odwQ.png)

```
# Display the head of the dataset
data.head()
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1FfcVtr0EjsggqhFpQE9kkA.png)

### Selecting Samples

In order to understand better our dataset and how the data will be transformed through the analysis, we will select a few sample points and explore them in detail.

```
# Select three indices to sample from the dataset
indices = [85,181,338]

# Create a DataFrame of the chosen samples
samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)
print("Chosen samples of wholesale customers dataset:")
display(samples)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1G3Iycv3VuXO9cPKZbB_RVw.png)

### Considerations

Let us consider now the total purchase cost of each product category and the statistical description of the dataset above for our sample customers. If we would have to predict what kind of establishment (customer) each of the three samples chosen represent:

Considering the mean values:

- Fresh: 12000.2977
- Milk: 5796.2
- Grocery: 3071.9
- Detergents\_paper: 2881.4
- Delicatessen: 1524.8

We can make the following predictions:

**1) Index 85: Retailer:**

- Largest spending on detergents and paper and groceries of the entire dataset, which usually are products for houses.
- Higher than average spending on milk.
- Lower than average spending on frozen products.

**2) Index 181: Large market**

- High spending on almost every product category.
- Highest spending on fresh products of the entire dataset. Likely to be a large market.
- Low spending on detergents.

**3) Index 338: Restaurant**

- The amount of every product is significantly lower than the previous two customers considered.
- The spending on fresh products is the lowest of the entire dataset.
- The spending on milk and detergent and papers is in the bottom quartile.
- It may be a small and cheap restaurant which needs groceries and frozen food to serve the meals.

### Feature Relevance

We will now analyze the relevance of the features for understanding the purchasing behaviours of the customers. In other words, to determine if a customer that purchase some amount of one category of products will necessarily purchase some proportional amount of another category of products.

We will study this by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.

```
# Display the head of the dataset
data.head(1)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1uatQYocvDj9M7Y7A7A5O-w.png)

```
# Make a copy of the DataFrame, using the 'drop' function to drop the given feature
new_data = data.drop('Grocery', axis=1)

# Split the data into training and testing sets(0.25) using the given feature as the target
# Set a random state.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(new_data, data.Grocery, test_size=0.25, random_state=42)

# Create a decision tree regressor and fit it to the training set
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor()
regressor = regressor.fit(X_train, y_train)
prediction = regressor.predict(X_test)

# Report the score of the prediction using the testing set
from sklearn.metrics import r2_score
score = r2_score(y_test, prediction)
print("Prediction score is: {}".format(score))
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1l5E-etVVhmJl9G0ZvL4sZQ.png)

- We tried to predict the Grocery feature.
- The reported prediction score was 67.25%.
- As we obtained a high score, it as indicator of a very good fit. So this feature is easy to predict considering the rest of spendign habits and, therefore, not very necessary for identifying customers’ spending habits.

### Visualizing Feature Distributions

In order to understand better our dataset, we will display a scatter matrix of every product feature.

The product features that show a correlation in the scatter matrix will be relevant to predict others.

```
# Produce a scatter matrix for each pair of features in the data
pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
```

![@](https://towardsdatascience.com/wp-content/uploads/2019/04/16wbmENmXMTFx-m3kVngIQw.png)@

```
# Display a correlation matrix
import seaborn as sns
sns.heatmap(data.corr())
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1L94o_lXELcvM9sBzZQeEEQ.png)

Using the scatter matrix and the correlation matrix as a references, we can infer the following:

- Data is not normally distributed, it is positively skewed and it resemeble the log-normal distribution.
- In most plots, most data points lie near the origin which shows little correlation between them.
- From the scatter plots and the heatmap of correlation, we can see that there is a strong correlation between the ‘Grocery’ and ‘Detergent\_paper’ features. The features ‘Grocery’ and ‘Milk’ also show a good degree of correlation.
- This correlation confirms my guess about the relevance of the ‘Grocery’ feature, which can be accurately predicted with the ‘Detergent\_paper’ feature. And, therefore, is not an absolutely necessary feature in the dataset.

## Data Preprocessing

This step is crucial to make sure that the results obtained are significant, meaningful and they are optimized. We will preprocess data by scaling it and detecting potential outliers.

### Feature Scaling

Usually, when data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http://econbrowser.com/archives/2014/02/use-of-logarithms-in-economics) to apply a non-linear scaling – particularly for financial data.

One way to achieve this scaling is by using a [Box-Cox test](http://scipy.github.io/devdocs/generated/scipy.stats.boxcox.html), which calculates the best power transformation of the data that reduces skewness. A simpler approach which can work in most cases would be applying the natural logarithm.

```
# Scale the data using the natural logarithm
log_data = np.log(data)
```

```
# Scale the sample data using the natural logarithm
log_samples = np.log(samples)
```

```
# Produce a scatter matrix for each pair of newly-transformed features
pd.scatter_matrix(log_data, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1LQE8q0sHxAwkGB2ZErrbcw.png)

### Observation

After applying a natural logarithm scaling to the data, the distribution of each feature appear much more normal. For any pairs of features we have identified earlier as being correlated, we observe here that correlation is still present (and whether it is now stronger or weaker than before).

Displaying the actual data:

```
# Display the log-transformed sample data
display(log_samples)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1yuy3DlzclV31J8GsvIExdA.png)

### Outlier Detection

Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points.

Here, we will use [Tukey’s Method for identfying outliers](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/): An _outlier step_ is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.

```
outliers = []

# For each feature find the data points with extreme high or low values
for feature in log_data.keys():

   # Calculate Q1 (25th percentile of the data) for the given feature
    Q1 = np.percentile(log_data[feature],25)

    # Calculate Q3 (75th percentile of the data) for the given feature
    Q3 = np.percentile(log_data[feature],75)

    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)
    step = 1.5 * (Q3-Q1)

    # Display the outliers
    print("Data points considered outliers for the feature '{}':".format(feature))
    display(log_data[~((log_data[feature] >= Q1 - step) &amp; (log_data[feature] <= Q3 + step))])
    lista = log_data[~((log_data[feature] >= Q1 - step) &amp; (log_data[feature] <= Q3 + step))].index.tolist()
    outliers.append(lista)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1qwW13iAhOkaDEnco5oaiXw.png)![](https://towardsdatascience.com/wp-content/uploads/2019/04/1kra3CeQovWnukS5o-sZ9DA.png)![](https://towardsdatascience.com/wp-content/uploads/2019/04/1AYjv0stL5kHRnIFPXKpXew.png)

```
outliers
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1mnNCmYgw3eRWW84hzQ66NQ.png)

```
# Detecting outliers that appear in more than one product
seen = {}
dupes = []

for lista in outliers:
    for index in lista:
        if index not in seen:
            seen[index] = 1
        else:
            if seen[index] == 1:
                dupes.append(index)
            seen[index] += 1
dupes = sorted(dupes)
dupes
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1XGMVRHS0uvghVb7eyTNw4g.png)

```
# Removing outliers
good_data = log_data.drop(dupes, axis=0).reset_index(drop=True)
```

### Observations

- Datapoints considered outliers that are present in more than one feature are: 65, 66, 75, 128, 154.
- K-Means is heavily influenced by the presence of outliers as they increase significantly the loss function that the algorithm tries to minimize. This loss function is the squared sum of the distances of each datapoint to the centroid, so, if the outlier is far enough, the centroid will be incorrectly situated. Because of this, the outliers shoul be removed.

## Feature Transformation

Now we will use Principal Component Analysis (PCA) to extract conclusions about the hidden structure of the dataset. PCA is used to calculate those dimensions that maximize variance, so we will find the combination of features that describe best each customer.

### Principal Component Analysis (PCA)

Once the data has been scaled to a normal distribution and the necessary outliers have been removed, we can apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved.

In addition to finding these dimensions, PCA will also report the _explained variance ratio_ of each dimension – how much variance within the data is explained by that dimension alone.

```
# Get the shape of the log_samples
log_samples.shape
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1PuSuZ1mI_uQIKlXmz_0EXw.png)

```
# Apply PCA by fitting the good data with the same number of dimensions as features
from sklearn.decomposition import PCA
pca = PCA(n_components=good_data.shape[1])
pca = pca.fit(good_data)

# Transform log_samples using the PCA fit above
pca_samples = pca.transform(log_samples)

# Generate PCA results plot
pca_results = vs.pca_results(good_data, pca)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1cAdrW8H5AjnafmgHp9QQiA.png)

### Observations

- The variance explained by the first two Principal Components is the 70.68% of the total.
- The variance explained by the first three Principal Components is the 93.11% of the total.

**Dimensions discussion**

- Dimension 1: This dimension represents well, in terms of negative variance, the following features: Detergent\_Paper, Milk and groceries. Mostly utilities for everyday consuming.
- Dimension 2: This dimension represents well, in terms of negative variance, the following features: Fresh, Frozen and Delicatessen. Mostly food consuming.
- Dimension 3: This dimension represents well, in terms of positive variance, the Delicatessen features, and in terms of negative variance de Fresh feature. Food to be consumed on the day.
- Dimension 4: This dimension represents well, in terms of positive variance, the Frozen feature, and in terms of negative variance, the Delicatessen Feature. Food that can be storaged.

```
# Display sample log-data after having a PCA transformation applied
display(pd.DataFrame(np.round(pca_samples, 4), columns = pca_results.index.values))
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1tPPwOjD4SNVeiejRE_VT5g.png)

### Dimensionality Reduction

When using principal component analysis, one of the main goals is to reduce the dimensionality of the data.

Dimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the _cumulative explained variance ratio_ is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a signifiant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.

```
# Apply PCA by fitting the good data with only two dimensions
pca = PCA(n_components=2).fit(good_data)

# Transform the good data using the PCA fit above
reduced_data = pca.transform(good_data)

# Transform log_samples using the PCA fit above
pca_samples = pca.transform(log_samples)

# Create a DataFrame for the reduced data
reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])
```

The cell below show how the log-transformed sample data has changed after having a PCA transformation applied to it using only two dimensions. Observe how the values for the first two dimensions remains unchanged when compared to a PCA transformation in six dimensions.

```
# Display sample log-data after applying PCA transformation in two dimensions
display(pd.DataFrame(np.round(pca_samples, 4), columns = ['Dimension 1', 'Dimension 2']))
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1ZH05rJ4nhx9dhbUOTY9elw.png)

### Visualizing a Biplot

A biplot is a scatterplot where each data point is represented by its scores along the principal components. The axes are the principal components (in this case `Dimension 1` and `Dimension 2`).

The biplot shows the projection of the original features along the components. A biplot can help us interpret the reduced dimensions of the data, and discover relationships between the principal components and original features.

```
# Create a biplot
vs.biplot(good_data, reduced_data, pca)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1hR6dGSxtVaSgWKHGepcGSA.png)

Once we have the original feature projections (in red), it is easier to interpret the relative position of each data point in the scatterplot.

For instance, a point the lower right corner of the figure will likely correspond to a customer that spends a lot on `'Milk'`, `'Grocery'`and `'Detergents_Paper'`, but not so much on the other product categories.

## Clustering

In this section, we will choose to use either a K-Means clustering algorithm or a Gaussian Mixture Model (GMM) clustering algorithm to identify the various customer segments hidden in the data.

We will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale.

**K-Means vs GMM**

1) The main advantages of using K-Means as a cluster algorithm are:

- It is easy to implement.
- With large number of variables, if (K is small), it may be computationally faster than hierarchichal clustering.
- Consistent and scale-invariant.
- It is guaranteed to converge.

2) The main advantages of using Gaussian Mixture Models as a cluster algorithm are:

- It is much more flexible in terms of cluster covariance. Which means that each cluster can have unconstrained covariance structure. In other words, whereas K-means assumes that every cluster have spherical estructure, GMM allows eliptical.
- Points can belong to different clusters, with different level of memebership. This level of membership is the probability of each point to belong to each cluster.

3) Chosen algorithm:

- The chosen algorithm is Gaussian Mixture Model. Because data is not splitted in clear and different clusters, so we do not know how many clusters there are.

### Creating Clusters

When the number of clusters is not known _a priori_, there is no guarantee that a given number of clusters best segments the data, since it is unclear what structure exists in the data.

However, we can quantify the "goodness" of a clustering by calculating each data point’s \_ [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\_. The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the _mean_ silhouette coefficient provides for a simple scoring method of a given clustering.

```
# Import the necessary libraries
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

scores = {}
for i in range(2,7):

    print('Number of clusters: ' + str(i))

    # Apply your clustering algorithm of choice to the reduced data
    clusterer = GaussianMixture(random_state=42, n_components=i)
    clusterer.fit(reduced_data)

    # Predict the cluster for each data point
    preds = clusterer.predict(reduced_data)

    # Find the cluster centers
    centers = clusterer.means_
    print('Cluster Center: ' + str(centers))

    # Predict the cluster for each transformed sample data point
    sample_preds = clusterer.predict(pca_samples)
    print('Sample predictions: ' + str(sample_preds))

    # Calculate the mean silhouette coefficient for the number of clusters chosen
    score = silhouette_score(reduced_data, preds)
    scores[i] = score
    print('Silhouette score is: ' + str(score), 'n')

print('Scores: ' + str(scores))
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1VvRKt9wgLOY7NJ71O74ksA.png)![](https://towardsdatascience.com/wp-content/uploads/2019/04/10dHLpEQtKITzk2bH5xR9ow.png)

The number of cluster with the best Silhouette Score is 2, with a score of 0.42.

### Cluster Visualization

Once we’ve chosen the optimal number of clusters for the clustering algorithm using the scoring metric above, we can now visualize the results in the code block below.

```
# Apply your clustering algorithm of choice to the reduced data
clusterer = GaussianMixture(random_state=42, n_components=2)
clusterer.fit(reduced_data)

# Predict the cluster for each data point
preds = clusterer.predict(reduced_data)

# Find the cluster centers
centers = clusterer.means_
print('Cluster Center: ' + str(centers))

# Predict the cluster for each transformed sample data point
sample_preds = clusterer.predict(pca_samples)
print('Sample predictions: ' + str(sample_preds))

# Calculate the mean silhouette coefficient for the number of clusters chosen
score = silhouette_score(reduced_data, preds)
scores[i] = score
print('Silhouette score is: ' + str(score), 'n')
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1-g2ufku2ZxVUkKhBdRYM0w.png)

```
# Display the results of the clustering from implementation
vs.cluster_results(reduced_data, preds, centers, pca_samples)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1Ehyk7g70ufiju7A4fT1yzQ.png)

### Data Recovery

Each cluster present in the visualization above has a central point. These centers (or means) are not specifically data points from the data, but rather the _averages_ of all the data points predicted in the respective clusters.

For the problem of creating customer segments, a cluster’s center point corresponds to _the average customer of that segment_. Since the data is currently reduced in dimension and scaled by a logarithm, we can recover the representative customer spending from these data points by applying the inverse transformations.

```
# Inverse transform the centers
log_centers = pca.inverse_transform(centers)

# Exponentiate the centers
true_centers = np.exp(log_centers)

# Display the true centers
segments = ['Segment {}'.format(i) for i in range(0,len(centers))]
true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())
true_centers.index = segments
display(true_centers)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/10VUnHCB5EUJ94oXi-eBWXg.png)

- Segment 0 may represent a a fresh food market as every feature except Frozen and Fresh are below the median.
- Segment 1 may represent a supermarket as every feature except fresh and frozen are above the median.

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1I1fIj0a9eLl56fqvk8LrVQ.png)

The code below shows to which ckustr each sample point is predicted ot belong.

```
# Display the predictions
for i, pred in enumerate(sample_preds):
    print("Sample point", i, "predicted to be in Cluster", pred)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1yyOD33PEyVkc_4Q_zTslrw.png)

**Observations**

- Sample point 0 → Supermarket and the original guess was a retailer. This difference may be explained because of the size of the cluster (which is pretty big)
- Sample point 1 → Supermarket and the originak guess was the same.
- Sample point 2 → Fresh food market and the original guess was a restaurant which is reasonable considering the amount of the spending of the features.

## Conclusion

_How can the wholesale distributor label the new customers using only their estimated product spending and the **customer segment** data?_

A supervised learning algorithm could be used with the estimated product spending as attributes and the customer segment as the target variable, making it a classification problem (we would have 2 possible labels). As there is not a clear mathematical relationship between the customer segment and the product spending KNN could be a good algorithm to work with.

### Visualizing Underlying Distributions

At the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset.

The code block below shows how each data point is labeled either `'HoReCa'` (Hotel/Restaurant/Cafe) or `'Retail'` the reduced space.

```
# Display the clustering results based on 'Channel' data vs.channel_results(reduced_data, preds, pca_samples)
```

![](https://towardsdatascience.com/wp-content/uploads/2019/04/1lCaAiiDwmWUKv7nG2m9rWw.png)

We can observe that the cluster algorithm does a pretty good job of clustering the data to the underlying distribution as the cluster 0 can be associated perfectly with a retailer and the cluster 1 to the Ho/Re/Ca.

## Final Words

As always, I hope you \*\*\*\* enjoyed the post, that you are now a pro on neural networks!

_If you want to learn more about Machine Learning, Data Science and Artificial Intelligence **follow me on Medium**, and stay tuned for my next posts!_

* * *

Written By

Victor Roman

[See all from Victor Roman](https://towardsdatascience.com/author/rromanss23/)

[Artificial Intelligence](https://towardsdatascience.com/tag/artificial-intelligence/), [Clustering](https://towardsdatascience.com/tag/clustering/), [Data Science](https://towardsdatascience.com/tag/data-science/), [Machine Learning](https://towardsdatascience.com/tag/machine-learning/), [Unsupervised Learning](https://towardsdatascience.com/tag/unsupervised-learning/)

Share This Article

- [Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-project-creating-customer-segments-17c4b4bbf925%2F&title=Unsupervised%20Learning%20Project%3A%20Creating%20Customer%20Segments)
- [Share on LinkedIn](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-project-creating-customer-segments-17c4b4bbf925%2F&title=Unsupervised%20Learning%20Project%3A%20Creating%20Customer%20Segments)
- [Share on X](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Funsupervised-learning-project-creating-customer-segments-17c4b4bbf925%2F&text=Unsupervised%20Learning%20Project%3A%20Creating%20Customer%20Segments)

Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.

[Write for TDS](https://towardsdatascience.com/questions-96667b06af5/)

## Related Articles

- ![](https://towardsdatascience.com/wp-content/uploads/2024/08/0c09RmbCCpfjAbSMq.png)





## [Implementing Convolutional Neural Networks in TensorFlow](https://towardsdatascience.com/implementing-convolutional-neural-networks-in-tensorflow-bc1c4f00bd34/)

[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)





Step-by-step code guide to building a Convolutional Neural Network









[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)





August 20, 2024




6 min read

- ## [What Do Large Language Models “Understand”?](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77/)

[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)





A deep dive on the meaning of understanding and how it applies to LLMs









[Tarik Dzekman](https://towardsdatascience.com/author/tarikdzekman/)





August 21, 2024




31 min read

- ![Photo by Krista Mangulsone on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/0GyVVTbgotH-DhGPH-scaled.jpg)





## [How to Forecast Hierarchical Time Series](https://towardsdatascience.com/how-to-forecast-hierarchical-time-series-75f223f79793/)

[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)





A beginner’s guide to forecast reconciliation









[Dr. Robert Kübler](https://towardsdatascience.com/author/dr-robert-kuebler/)





August 20, 2024




13 min read

- ![Photo by davisuko on Unsplash](https://towardsdatascience.com/wp-content/uploads/2024/08/1bAABgtZtAIG5YW1oEjW3pA-scaled.jpeg)





## [Hands-on Time Series Anomaly Detection using Autoencoders, with Python](https://towardsdatascience.com/hands-on-time-series-anomaly-detection-using-autoencoders-with-python-7cd893bbc122/)

[Data Science](https://towardsdatascience.com/category/data-science/)





Here’s how to use Autoencoders to detect signals with anomalies in a few lines of…









[Piero Paialunga](https://towardsdatascience.com/author/piero-paialunga/)





August 21, 2024




12 min read

- ![Image from Canva.](https://towardsdatascience.com/wp-content/uploads/2024/08/1UAA9jQVdqMXnwzYiz8Q53Q.png)





## [3 AI Use Cases (That Are Not a Chatbot)](https://towardsdatascience.com/3-ai-use-cases-that-are-not-a-chatbot-f4f328a2707a/)

[Machine Learning](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)





Feature engineering, structuring unstructured data, and lead scoring









[Shaw Talebi](https://towardsdatascience.com/author/shawhin/)





August 21, 2024




7 min read

- ## [Solving a Constrained Project Scheduling Problem with Quantum Annealing](https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/)

[Data Science](https://towardsdatascience.com/category/data-science/)





Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM)









[Luis Fernando PÉREZ ARMAS, Ph.D.](https://towardsdatascience.com/author/luisfernandopa1212/)





August 20, 2024




29 min read

- ![](https://towardsdatascience.com/wp-content/uploads/2023/02/1VEUgT5T4absnTqBMOEuNig.png)





## [Back To Basics, Part Uno: Linear Regression and Cost Function](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46/)

[Data Science](https://towardsdatascience.com/category/data-science/)





An illustrated guide on essential machine learning concepts









[Shreya Rao](https://towardsdatascience.com/author/shreya-rao/)





February 3, 2023




6 min read


Some areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.